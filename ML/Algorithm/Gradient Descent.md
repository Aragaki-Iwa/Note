# 梯度下降  
梯度，就是求偏导。多元函数对每个自变量求偏导形成一个向量，即为梯度。  
*性质*：可到函数 在某一点取得极值的 必要条件是 梯度为0，梯度为0的点称为 驻点。换言之，并不是梯度为0就是极值点，只是有这个可能。  
     极大值还是极小值？看二阶导数，即Hessian矩阵——有二阶偏导构成的矩阵。
     Hessian矩阵—— a)正定——>函数有极小值  
                  b)负定——>函数有极大值  
                  c)不定——>不定（不是极值点，是鞍点）
    梯度下降就是沿着梯度的负方向来找函数的极小值。  
    之所以是负方向，是因为，类如：导数越大，函数增长越快，所以沿着其负方向就能越快下降。  
    步长（学习率）：步幅过大，错过极小值；过小，收敛慢。（可以尝试多种策略）  
    可能遇到的问题：局部极小/鞍点（梯度为0，但Hessian矩阵不定的点） 
                  应对措施：随机
    改进（即变种）：利用之前迭代的梯度信息来更新    
    "深度学习中的常用训练算法"http://xudongyang.coding.me/gradient-descent-variants/
